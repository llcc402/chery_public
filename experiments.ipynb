{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from utilities import *\n",
    "from models import HMCModel, HMC_LSTM, CoherentLSTM\n",
    "from train_model_utilities import * \n",
    "import os \n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate simple data and test 3 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data \n",
    "N = 200\n",
    "data1 = generate_rectangular_data(3, 4, 2, 3, size=N)\n",
    "data2 = generate_rectangular_data(6, 8, 4, 5, size=N)\n",
    "data = np.concatenate([data1, data2])\n",
    "y1 = np.array([0.0]*N + [1.0]*N)\n",
    "y2 = np.array([1.0]*N + [0.0]*N)\n",
    "y3 = np.ones([N*2,1])\n",
    "y = np.concatenate([y1[:,np.newaxis], y2[:,np.newaxis], y3], axis=1)\n",
    "structure = np.array([[0,0,1],[0,0,1],[0,0,0]])\n",
    "\n",
    "(x_train, y_train), (x_valid, y_valid), (x_test, y_test) = split_train_valid_test(data, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 400\n",
    "learning_rate=1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40/400, loss 0.24104656279087067, valid loss 0.24391953647136688\n",
      "epoch 80/400, loss 0.04617023840546608, valid loss 0.055352583527565\n",
      "epoch 120/400, loss 0.02191001921892166, valid loss 0.024649979546666145\n",
      "epoch 160/400, loss 0.023621829226613045, valid loss 0.016323549672961235\n",
      "epoch 200/400, loss 0.0059523796662688255, valid loss 0.011375175788998604\n",
      "epoch 240/400, loss 0.009899633005261421, valid loss 0.00946457777172327\n",
      "epoch 280/400, loss 0.011181006208062172, valid loss 0.007882690988481045\n",
      "epoch 320/400, loss 0.0019166248384863138, valid loss 0.0062451232224702835\n",
      "epoch 360/400, loss 0.0038437391631305218, valid loss 0.005041854456067085\n",
      "epoch 400/400, loss 0.002166174352169037, valid loss 0.00398711021989584\n",
      "AU(PRC) \n",
      "train: 0.9999978343356588, validation 1.0, test 1.0\n"
     ]
    }
   ],
   "source": [
    "model1 = HMCModel(structure, 3, [10])\n",
    "loss_fn = get_loss_fn_coherent(structure)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train, y_train)\n",
    ").shuffle(buffer_size = N*2).batch(batch_size=batch_size)\n",
    "train_step_fn = get_train_step_fn(model1, optimizer, loss_fn, 'coherent_hmc')\n",
    "for epoch in range(epochs):\n",
    "    for x_train_batch, y_train_batch in train_data:\n",
    "        loss, valid_loss = train_step_fn(\n",
    "            x_train_batch, \n",
    "            y_train_batch, \n",
    "            (x_valid, y_valid)\n",
    "        )\n",
    "    if (epoch + 1) % (epochs // 10) == 0:\n",
    "        print(\"epoch {}/{}, loss {}, valid loss {}\".format(epoch+1, epochs, loss, valid_loss))\n",
    "        \n",
    "print_model_eval_score(model1, 'coherent_hmc', x_train, y_train, x_valid, y_valid, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40/400, loss 0.12798011302947998, valid loss 0.13868382573127747\n",
      "epoch 80/400, loss 0.03187961131334305, valid loss 0.034657757729291916\n",
      "epoch 120/400, loss 0.02332611009478569, valid loss 0.01955314353108406\n",
      "epoch 160/400, loss 0.023429105058312416, valid loss 0.011818751692771912\n",
      "epoch 200/400, loss 0.005147560499608517, valid loss 0.008377335034310818\n",
      "epoch 240/400, loss 0.012806359678506851, valid loss 0.007442173082381487\n",
      "epoch 280/400, loss 0.0024359598755836487, valid loss 0.004803475458174944\n",
      "epoch 320/400, loss 0.0035969861783087254, valid loss 0.004092950839549303\n",
      "epoch 360/400, loss 0.009740754961967468, valid loss 0.004177312832325697\n",
      "epoch 400/400, loss 0.0021576527506113052, valid loss 0.00311973225325346\n",
      "AU(PRC) \n",
      "train: 1.0, validation 1.0, test 1.0\n"
     ]
    }
   ],
   "source": [
    "model2 = HMC_LSTM(10, 0.5, [2,1], 2)\n",
    "loss_fn = get_loss_fn_lstm(structure, 0.5, 0.5)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "train_step_fn = get_train_step_fn(model2, optimizer, loss_fn, 'lstm_hmc')\n",
    "for epoch in range(epochs):\n",
    "    for x_train_batch, y_train_batch in train_data:\n",
    "        loss, valid_loss = train_step_fn(\n",
    "            x_train_batch, \n",
    "            y_train_batch, \n",
    "            (x_valid, y_valid)\n",
    "        )\n",
    "    if (epoch + 1) % (epochs // 10) == 0:\n",
    "        print(\"epoch {}/{}, loss {}, valid loss {}\".format(epoch+1, epochs, loss, valid_loss))\n",
    "        \n",
    "print_model_eval_score(model2, 'lstm_hmc', x_train, y_train, x_valid, y_valid, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40/400, loss 0.06825626641511917, valid loss 0.07301826030015945\n",
      "epoch 80/400, loss 0.014876448549330235, valid loss 0.02218560315668583\n",
      "epoch 120/400, loss 0.0105137275531888, valid loss 0.012428480200469494\n",
      "epoch 160/400, loss 0.003024618374183774, valid loss 0.008746640756726265\n",
      "epoch 200/400, loss 0.006929394323378801, valid loss 0.0069747185334563255\n",
      "epoch 240/400, loss 0.003787405788898468, valid loss 0.0049867588095366955\n",
      "epoch 280/400, loss 0.0070106713101267815, valid loss 0.0038296966813504696\n",
      "epoch 320/400, loss 0.007498203311115503, valid loss 0.002827154705300927\n",
      "epoch 360/400, loss 0.008971057832241058, valid loss 0.002330430783331394\n",
      "epoch 400/400, loss 0.008304952643811703, valid loss 0.002916258294135332\n",
      "AU(PRC) \n",
      "train: 1.0, validation 1.0, test 1.0\n"
     ]
    }
   ],
   "source": [
    "model3 = CoherentLSTM(10, 0.5, [2,1], 2, structure)\n",
    "loss_fn = get_loss_fn_coherent(structure)\n",
    "optimizer = tf.keras.optimizers.Adam(1e-2)\n",
    "train_step_fn = get_train_step_fn(model3, optimizer, loss_fn, 'coherent_lstm')\n",
    "for epoch in range(epochs):\n",
    "    for x_train_batch, y_train_batch in train_data:\n",
    "        loss, valid_loss = train_step_fn(\n",
    "            x_train_batch, \n",
    "            y_train_batch, \n",
    "            (x_valid, y_valid)\n",
    "        )\n",
    "    if (epoch + 1) % (epochs // 10) == 0:\n",
    "        print(\"epoch {}/{}, loss {}, valid loss {}\".format(epoch+1, epochs, loss, valid_loss))\n",
    "        \n",
    "print_model_eval_score(model3, 'coherent_lstm', x_train, y_train, x_valid, y_valid, x_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate complex data and compare 3 models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid in a square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 800\n",
    "data = generate_rectangular_data(0,0,2,2,N)\n",
    "(x1,x2,y1,y2) = (-0.7, 0.7, -0.7, 0.7)\n",
    "label_int_2_name = {0:'all', 1:'left', 2:'right', 3:'upper', 4:'lower', 5:'upper left', \n",
    "                    6:'upper right', 7:'lower left', 8:'lower right'}\n",
    "\n",
    "def assign_label(data, x1, x2, y1, y2):\n",
    "    y = np.zeros([N, 9])\n",
    "    y[:,0] = 1\n",
    "    y[:,1] = np.where(data[:,0] <= x1, 1, 0)\n",
    "    y[:,2] = np.where(data[:,0] > x2, 1, 0)\n",
    "    y[:,3] = np.where(data[:,1] > y2, 1, 0)\n",
    "    y[:,4] = np.where(data[:,1] <= y1, 1, 0)\n",
    "    y[:,5] = np.where((data[:,0] <= x1) & (data[:,1] > y2), 1, 0)\n",
    "    y[:,6] = np.where((data[:,0] > x2) & (data[:,1] > y2), 1, 0)\n",
    "    y[:,7] = np.where((data[:,0] <= x1) & (data[:,1] <= y1), 1, 0)\n",
    "    y[:,8] = np.where((data[:,0] > x2) & (data[:,1] <= y1), 1, 0)\n",
    "    return y \n",
    "\n",
    "y = assign_label(data, x1, x2, y1, y2)\n",
    "structure = np.array([\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [1, 1, 0, 1, 0, 0, 0, 0, 0],\n",
    "    [1, 0, 1, 1, 0, 0, 0, 0, 0],\n",
    "    [1, 1, 0, 0, 1, 0, 0, 0, 0],\n",
    "    [1, 0, 1, 0, 1, 0, 0, 0, 0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_valid, y_valid), (x_test, y_test) = split_train_valid_test(data, y)\n",
    "\n",
    "batch_size = 64\n",
    "train_data = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train, y_train)\n",
    ").shuffle(buffer_size = N).batch(batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20/200, loss 0.10874809324741364, valid loss 0.08516314625740051\n",
      "epoch 40/200, loss 0.046086449176073074, valid loss 0.058226246386766434\n",
      "epoch 60/200, loss 0.04298800230026245, valid loss 0.05066411942243576\n",
      "epoch 80/200, loss 0.04981210082769394, valid loss 0.0471133328974247\n",
      "epoch 100/200, loss 0.03852337598800659, valid loss 0.04379596188664436\n",
      "epoch 120/200, loss 0.04977623000741005, valid loss 0.04235325753688812\n",
      "epoch 140/200, loss 0.034783586859703064, valid loss 0.0419049970805645\n",
      "epoch 160/200, loss 0.05221181362867355, valid loss 0.03959105908870697\n",
      "epoch 180/200, loss 0.05668099597096443, valid loss 0.04171271622180939\n",
      "epoch 200/200, loss 0.04271060973405838, valid loss 0.03882627934217453\n",
      "AU(PRC) \n",
      "train: 0.9950568170453984, validation 0.9931325751826723, test 0.9936009540774329\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-2\n",
    "epochs = 200 \n",
    "model1 = HMCModel(structure, 9, [20])\n",
    "loss_fn = get_loss_fn_coherent(structure)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "train_step_fn = get_train_step_fn(model1, optimizer, loss_fn, 'coherent_hmc')\n",
    "for epoch in range(epochs):\n",
    "    for x_train_batch, y_train_batch in train_data:\n",
    "        loss, valid_loss = train_step_fn(\n",
    "            x_train_batch, \n",
    "            y_train_batch, \n",
    "            (x_valid, y_valid)\n",
    "        )\n",
    "    if (epoch + 1) % (epochs // 10) == 0:\n",
    "        print(\"epoch {}/{}, loss {}, valid loss {}\".format(epoch+1, epochs, loss, valid_loss))\n",
    "        \n",
    "print_model_eval_score(model1, 'coherent_hmc', x_train, y_train, x_valid, y_valid, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20/200, loss 0.16037175059318542, valid loss 0.1503029763698578\n",
      "epoch 40/200, loss 0.048256974667310715, valid loss 0.055528365075588226\n",
      "epoch 60/200, loss 0.0314185693860054, valid loss 0.033141396939754486\n",
      "epoch 80/200, loss 0.0269512627273798, valid loss 0.029593585059046745\n",
      "epoch 100/200, loss 0.029599834233522415, valid loss 0.023017430678009987\n",
      "epoch 120/200, loss 0.011349653825163841, valid loss 0.022566748782992363\n",
      "epoch 140/200, loss 0.01314354594796896, valid loss 0.02277875505387783\n",
      "epoch 160/200, loss 0.012446343898773193, valid loss 0.01662961207330227\n",
      "epoch 180/200, loss 0.007814988493919373, valid loss 0.025553222745656967\n",
      "epoch 200/200, loss 0.010632765479385853, valid loss 0.01981385610997677\n",
      "AU(PRC) \n",
      "train: 0.9999992644793849, validation 0.9977912531047508, test 0.999993266531364\n"
     ]
    }
   ],
   "source": [
    "model2 = HMC_LSTM(10, 0.5, [1, 4, 4], 3)\n",
    "loss_fn = get_loss_fn_lstm(structure, 0.5, 0.5)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "train_step_fn = get_train_step_fn(model2, optimizer, loss_fn, 'lstm_hmc')\n",
    "for epoch in range(epochs):\n",
    "    for x_train_batch, y_train_batch in train_data:\n",
    "        loss, valid_loss = train_step_fn(\n",
    "            x_train_batch, \n",
    "            y_train_batch, \n",
    "            (x_valid, y_valid)\n",
    "        )\n",
    "    if (epoch + 1) % (epochs // 10) == 0:\n",
    "        print(\"epoch {}/{}, loss {}, valid loss {}\".format(epoch+1, epochs, loss, valid_loss))\n",
    "        \n",
    "print_model_eval_score(model2, 'lstm_hmc', x_train, y_train, x_valid, y_valid, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20/200, loss 0.1717752069234848, valid loss 0.1616838276386261\n",
      "epoch 40/200, loss 0.09314855933189392, valid loss 0.0876845121383667\n",
      "epoch 60/200, loss 0.057259202003479004, valid loss 0.0652986690402031\n",
      "epoch 80/200, loss 0.06603892147541046, valid loss 0.060226909816265106\n",
      "epoch 100/200, loss 0.04537113755941391, valid loss 0.05114007741212845\n",
      "epoch 120/200, loss 0.04695947840809822, valid loss 0.04253528267145157\n",
      "epoch 140/200, loss 0.032057903707027435, valid loss 0.04649890959262848\n",
      "epoch 160/200, loss 0.04562141001224518, valid loss 0.042584940791130066\n",
      "epoch 180/200, loss 0.059920407831668854, valid loss 0.04141862690448761\n",
      "epoch 200/200, loss 0.019298110157251358, valid loss 0.039662498980760574\n",
      "AU(PRC) \n",
      "train: 0.9999734926486514, validation 0.9989902114584428, test 0.999097376549777\n"
     ]
    }
   ],
   "source": [
    "model3 = CoherentLSTM(10, 0.5, [1, 4, 4], 3, structure)\n",
    "loss_fn = get_loss_fn_coherent(structure)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "train_step_fn = get_train_step_fn(model3, optimizer, loss_fn, 'coherent_lstm')\n",
    "for epoch in range(epochs):\n",
    "    for x_train_batch, y_train_batch in train_data:\n",
    "        loss, valid_loss = train_step_fn(\n",
    "            x_train_batch, \n",
    "            y_train_batch, \n",
    "            (x_valid, y_valid)\n",
    "        )\n",
    "    if (epoch + 1) % (epochs // 10) == 0:\n",
    "        print(\"epoch {}/{}, loss {}, valid loss {}\".format(epoch+1, epochs, loss, valid_loss))\n",
    "        \n",
    "print_model_eval_score(model3, 'coherent_lstm', x_train, y_train, x_valid, y_valid, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13 (default, Oct 19 2022, 10:19:43) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b118cd4708a217f676eace989c7f65c28bb1737b4d79cbde311a23c592f7bda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
