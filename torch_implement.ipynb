{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import datasets\n",
    "from utils.parser import * \n",
    "import os \n",
    "import numpy as np \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import networkx as nx \n",
    "import torch\n",
    "from torch import nn \n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_structure_from_adajancency(adajancency):\n",
    "    structure = np.zeros(adajancency.shape)\n",
    "    g = nx.DiGraph(adajancency) # train.A is the matrix where the direct connections are stored \n",
    "    for i in range(len(adajancency)):\n",
    "        ancestors = list(nx.descendants(g, i)) #here we need to use the function nx.descendants() because in the directed graph the edges have source from the descendant and point towards the ancestor \n",
    "        if ancestors:\n",
    "            structure[i, ancestors] = 1\n",
    "    return structure \n",
    "\n",
    "def mymax(prob, R):\n",
    "    R1 = (R + np.eye(R.shape[1])).astype(np.float32)\n",
    "    R1 = torch.from_numpy(R1).unsqueeze(0)\n",
    "    prob1 = prob.unsqueeze(-1)\n",
    "    prob1, idx = torch.max(R1 * prob1, dim=1)\n",
    "    return prob1\n",
    "\n",
    "dataset_name = 'seq_FUN'\n",
    "train, valid, test = initialize_dataset(dataset_name, datasets)\n",
    "R = get_structure_from_adajancency(train.A)\n",
    "\n",
    "scalar = StandardScaler().fit(train.X)\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean').fit(train.X)\n",
    "train.X = scalar.transform(imputer.transform(train.X))\n",
    "valid.X = scalar.transform(imputer.transform(valid.X))\n",
    "test.X = scalar.transform(imputer.transform(test.X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMCModel(nn.Module):\n",
    "    def __init__(self, inp_dim, hid_dims, out_dim, drop_rate, R):\n",
    "        super(HMCModel, self).__init__()\n",
    "        self.hid_dims = hid_dims\n",
    "        self.R = R \n",
    "        self.W = list()\n",
    "        for i in range(len(hid_dims)):\n",
    "            if i == 0:\n",
    "                self.W.append(nn.Linear(inp_dim, hid_dims[i]))\n",
    "            else:\n",
    "                self.W.append(nn.Linear(hid_dims[i-1], hid_dims[i]))\n",
    "        self.W.append(nn.Linear(hid_dims[-1], out_dim))\n",
    "        self.W = nn.ModuleList(self.W)\n",
    "        self.drop = nn.Dropout(drop_rate)\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.hid_dims)):\n",
    "            x = self.W[i](x)\n",
    "            x = nn.ReLU()(x)\n",
    "            x = self.drop(x)\n",
    "        x = self.W[-1](x)\n",
    "        x = nn.Sigmoid()(x)\n",
    "        if self.training:\n",
    "            outputs = x \n",
    "        else:\n",
    "            outputs = mymax(x, self.R)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.X = torch.tensor(train.X, dtype=torch.float32)\n",
    "valid.X = torch.tensor(valid.X, dtype=torch.float32)\n",
    "test.X = torch.tensor(test.X, dtype=torch.float32)\n",
    "train.Y = torch.tensor(train.Y)\n",
    "valid.Y = torch.tensor(valid.Y)\n",
    "test.Y = torch.tensor(test.Y)\n",
    "train.to_eval = torch.tensor(train.to_eval, dtype=torch.uint8),  \n",
    "test.to_eval = torch.tensor(test.to_eval, dtype=torch.uint8)\n",
    "\n",
    "train_dataset = [(x,y) for (x,y) in zip(train.X, train.Y)]\n",
    "val_dataset = [(x, y) for (x, y) in zip(valid.X, valid.Y)]\n",
    "for (x, y) in zip(valid.X, valid.Y):\n",
    "    train_dataset.append((x,y))\n",
    "test_dataset = [(x,y) for (x,y) in zip(test.X, test.Y)]\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                            batch_size=4, \n",
    "                                            shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                            batch_size=4, \n",
    "                                            shuffle=False)\n",
    "model = HMCModel(train.X.shape[1], [2000,2000], 500, 0.7, R)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(13):\n",
    "        model.train()\n",
    "\n",
    "        for i, (x, labels) in enumerate(train_loader):\n",
    "            # Clear gradients w.r.t. parameters\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x.float())\n",
    "\n",
    "            #MCLoss\n",
    "            constr_output = mymax(output, R)\n",
    "            train_output = labels*output.double()\n",
    "            train_output = mymax(train_output, R)\n",
    "            train_output = (1-labels)*constr_output.double() + labels*train_output\n",
    "\n",
    "            loss = nn.BCELoss()(train_output[:,train.to_eval[0]], labels[:,train.to_eval[0]]) \n",
    "\n",
    "            predicted = constr_output.data > 0.5\n",
    "\n",
    "            # Total number of labels\n",
    "            total_train = labels.size(0) * labels.size(1)\n",
    "            # Total correct predictions\n",
    "            correct_train = (predicted == labels.byte()).sum()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "for i, (x,y) in enumerate(test_loader):\n",
    "    model.eval()\n",
    "\n",
    "    constrained_output = model(x.float())\n",
    "    predicted = constrained_output.data > 0.5\n",
    "    # Total number of labels\n",
    "    total = y.size(0) * y.size(1)\n",
    "    # Total correct predictions\n",
    "    correct = (predicted == y.byte()).sum()\n",
    "\n",
    "    if i == 0:\n",
    "        predicted_test = predicted\n",
    "        constr_test = constrained_output\n",
    "        y_test = y\n",
    "    else:\n",
    "        predicted_test = torch.cat((predicted_test, predicted), dim=0)\n",
    "        constr_test = torch.cat((constr_test, constrained_output), dim=0)\n",
    "        y_test = torch.cat((y_test, y), dim =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2895981035027354\n"
     ]
    }
   ],
   "source": [
    "y_true = y_test[:,torch.where(test.to_eval == 1)[0].numpy()].numpy()\n",
    "y_pred = constr_test[:,torch.where(test.to_eval == 1)[0].numpy()].data.numpy()\n",
    "score = average_precision_score(y_true, y_pred, average='micro')\n",
    "print(score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model from paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_constr_out(x, R):\n",
    "    \"\"\" Given the output of the neural network x returns the output of MCM given the hierarchy constraint expressed in the matrix R \"\"\"\n",
    "    c_out = x.double()\n",
    "    c_out = c_out.unsqueeze(1)\n",
    "    c_out = c_out.expand(len(x),R.shape[1], R.shape[1])\n",
    "    R_batch = R.expand(len(x),R.shape[1], R.shape[1])\n",
    "    final_out, _ = torch.max(R_batch*c_out.double(), dim = 2)\n",
    "    return final_out\n",
    "\n",
    "\n",
    "class ConstrainedFFNNModel(nn.Module):\n",
    "    \"\"\" C-HMCNN(h) model - during training it returns the not-constrained output that is then passed to MCLoss \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, hyperparams, R):\n",
    "        super(ConstrainedFFNNModel, self).__init__()\n",
    "        \n",
    "        self.nb_layers = hyperparams['num_layers']\n",
    "        self.R = R\n",
    "        \n",
    "        fc = []\n",
    "        for i in range(self.nb_layers):\n",
    "            if i == 0:\n",
    "                fc.append(nn.Linear(input_dim, hidden_dim))\n",
    "            elif i == self.nb_layers-1:\n",
    "                fc.append(nn.Linear(hidden_dim, output_dim))\n",
    "            else:\n",
    "                fc.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "        self.fc = nn.ModuleList(fc)\n",
    "        \n",
    "        self.drop = nn.Dropout(hyperparams['dropout'])\n",
    "        \n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        if hyperparams['non_lin'] == 'tanh':\n",
    "            self.f = nn.Tanh()\n",
    "        else:\n",
    "            self.f = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i in range(self.nb_layers):\n",
    "            if i == self.nb_layers-1:\n",
    "                x = self.sigmoid(self.fc[i](x))\n",
    "            else:\n",
    "                x = self.f(self.fc[i](x))\n",
    "                x = self.drop(x)\n",
    "        if self.training:\n",
    "            constrained_out = x\n",
    "        else:\n",
    "            constrained_out = get_constr_out(x, self.R)\n",
    "        return constrained_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "num_layers = 3\n",
    "dropout = 0.7\n",
    "non_lin = 'relu'\n",
    "hidden_dim = 2000\n",
    "lr = 1e-4\n",
    "weight_decay = 1e-5\n",
    "num_epochs = 13\n",
    "hyperparams = {'batch_size':batch_size, 'num_layers':num_layers, 'dropout':dropout, 'non_lin':non_lin, 'hidden_dim':hidden_dim, 'lr':lr, 'weight_decay':weight_decay}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = initialize_dataset('seq_FUN', datasets)\n",
    "train.to_eval, val.to_eval, test.to_eval = torch.tensor(train.to_eval, dtype=torch.uint8), torch.tensor(val.to_eval, dtype=torch.uint8), torch.tensor(test.to_eval, dtype=torch.uint8)\n",
    "\n",
    "R = np.zeros(train.A.shape)\n",
    "np.fill_diagonal(R, 1)\n",
    "g = nx.DiGraph(train.A) # train.A is the matrix where the direct connections are stored \n",
    "for i in range(len(train.A)):\n",
    "    ancestors = list(nx.descendants(g, i)) #here we need to use the function nx.descendants() because in the directed graph the edges have source from the descendant and point towards the ancestor \n",
    "    if ancestors:\n",
    "        R[i, ancestors] = 1\n",
    "R = torch.tensor(R)\n",
    "#Transpose to get the descendants for each node \n",
    "R = R.transpose(1, 0)\n",
    "\n",
    "scaler = StandardScaler().fit(np.concatenate((train.X, val.X)))\n",
    "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean').fit(np.concatenate((train.X, val.X)))\n",
    "val.X, val.Y = torch.tensor(scaler.transform(imp_mean.transform(val.X))), torch.tensor(val.Y)\n",
    "train.X, train.Y = torch.tensor(scaler.transform(imp_mean.transform(train.X))), torch.tensor(train.Y)       \n",
    "test.X, test.Y = torch.tensor(scaler.transform(imp_mean.transform(test.X))), torch.tensor(test.Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = [(x, y) for (x, y) in zip(train.X, train.Y)]\n",
    "if ('others' not in args.dataset):\n",
    "    val_dataset = [(x, y) for (x, y) in zip(val.X, val.Y)]\n",
    "    for (x, y) in zip(val.X, val.Y):\n",
    "        train_dataset.append((x,y))\n",
    "test_dataset = [(x, y) for (x, y) in zip(test.X, test.Y)]\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                        batch_size=batch_size, \n",
    "                                        shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                        batch_size=batch_size, \n",
    "                                        shuffle=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9d1df8a9c631c41e95662a04b9faaa5fcedaf5ec92d88d24f4a9901426a87932"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
