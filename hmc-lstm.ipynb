{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.layers import Dense \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import os \n",
    "# os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.normal(size=[100,2])\n",
    "B = np.random.normal(size=[200,2], loc=1)\n",
    "data = np.concatenate([A,B])\n",
    "\n",
    "yA1 = np.array([1]*100 + [0]*200)\n",
    "yA2 = np.array([0]*100 + [1]*200)\n",
    "yB = np.ones(300)\n",
    "y = np.concatenate([yA1[:,np.newaxis], yA2[:,np.newaxis], yB[:,np.newaxis]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "def get_loss_fn(structure, alpha, beta):\n",
    "    ''' \n",
    "    structure     array of shape [M,M], where M is the number of all classes \n",
    "                  (a hierarchical level may contain multiple classes), \n",
    "                  structure[i,j] == 1 iff \"class i\" is a subclass of \"class j\"\n",
    "    alpha         a hyper-parameter to balance hierarchical loss and prediction loss\n",
    "    beta          a hyper-parameter to balance local prob and global prob\n",
    "    '''\n",
    "    def loss_fn(y_true, y_pred_logits):\n",
    "        ''' \n",
    "        y_true     [N,K], K = K1 + K2 + ... + KM, where M is the number of layers\n",
    "        y_pred     [total_prob, local_prob, global_prob], local_prob = [N,K], global_prob = [N,K]\n",
    "        '''\n",
    "        local_logits, global_logits = y_pred_logits \n",
    "        \n",
    "        local_loss = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(y_true, local_logits)\n",
    "        )\n",
    "        \n",
    "        global_loss = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(y_true, global_logits)\n",
    "        )\n",
    "        \n",
    "        total_prob = beta * tf.math.sigmoid(local_logits) + (1-beta) * tf.math.sigmoid(global_logits)\n",
    "        \n",
    "        hierachical_loss = 0\n",
    "        for i in range(structure.shape[0]):\n",
    "            for j in range(structure.shape[1]):\n",
    "                if structure[i,j] == 1:\n",
    "                    hierachical_loss += tf.reduce_sum(\n",
    "                        tf.where(\n",
    "                            total_prob[:,i] < total_prob[:,j], \n",
    "                            0, \n",
    "                            tf.pow(total_prob[:,i] - total_prob[:,j],2)\n",
    "                        )\n",
    "                    )\n",
    "        hierachical_loss /= y_true.shape[0]\n",
    "        \n",
    "        return local_loss + global_loss + alpha * hierachical_loss\n",
    "        \n",
    "    return loss_fn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMC_LSTM(tf.keras.Model):\n",
    "    def __init__(self, units, beta, num_classes_list, num_layers):\n",
    "        '''\n",
    "        units               number of nurons in LSTM\n",
    "        beta                integer, balance global and local predictions\n",
    "        num_classes_list    a list of number of classes in each layer\n",
    "        num_layers          integer, len(num_classes_list) == num_layers\n",
    "        '''\n",
    "        super(HMC_LSTM, self).__init__()\n",
    "        self.units = units \n",
    "        self.beta = beta \n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.input_gate = Dense(units,activation='sigmoid')\n",
    "        self.output_gate = Dense(units, activation='sigmoid')\n",
    "        self.forget_gate = Dense(units, activation='sigmoid')\n",
    "        self.candidate_gate = Dense(units, activation='tanh')\n",
    "        \n",
    "        self.local_W = list()\n",
    "        for i in range(self.num_layers):\n",
    "            self.local_W.append(Dense(num_classes_list[i]))\n",
    "        \n",
    "        self.global_W = Dense(tf.reduce_sum(num_classes_list))\n",
    "            \n",
    "    def call(self, inputs):\n",
    "        ''' \n",
    "        inputs     [N,k], where N is the number of examples\n",
    "        '''\n",
    "        \n",
    "        # init sequence\n",
    "        x = tf.concat(\n",
    "            [inputs, tf.zeros([inputs.shape[0],self.units])], \n",
    "            axis=-1\n",
    "        )\n",
    "        candidate_last = tf.zeros(self.units)\n",
    "        \n",
    "        # output probs\n",
    "        local_probs = list()\n",
    "        \n",
    "        # main loop\n",
    "        for i in range(self.num_layers):\n",
    "            # gates\n",
    "            forget_prob = self.forget_gate(x)\n",
    "            input_prob = self.input_gate(x)\n",
    "            output_prob = self.output_gate(x)\n",
    "            candidate_hat = self.candidate_gate(x)\n",
    "            \n",
    "            # new candidate\n",
    "            candidate = candidate_hat * input_prob + forget_prob * candidate_last\n",
    "            \n",
    "            # outputs\n",
    "            outputs = output_prob * tf.math.tanh(candidate)\n",
    "            \n",
    "            # update\n",
    "            candidate_last = candidate \n",
    "            x = tf.concat([inputs, outputs], axis=-1)\n",
    "\n",
    "            # local probs\n",
    "            local_probs.append(self.local_W[i](outputs))\n",
    "            \n",
    "        global_logits = self.global_W(\n",
    "            tf.concat([inputs, outputs], axis=-1)\n",
    "        )\n",
    "        local_logits = tf.concat(local_probs, axis=-1)\n",
    "        \n",
    "        return local_logits, global_logits\n",
    "    \n",
    "    def postprocess(self, local_logits, global_logits):\n",
    "        local_prob = tf.nn.sigmoid(local_logits)\n",
    "        global_prob = tf.nn.sigmoid(global_logits)\n",
    "        total_prob = self.beta * local_prob + (1-self.beta) * global_prob\n",
    "        return total_prob, local_prob, global_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 500/5000, loss = 0.5115510821342468\n",
      "epoch 1000/5000, loss = 0.4327765107154846\n",
      "epoch 1500/5000, loss = 0.3604480028152466\n",
      "epoch 2000/5000, loss = 0.316402405500412\n",
      "epoch 2500/5000, loss = 0.2916376292705536\n",
      "epoch 3000/5000, loss = 0.2715415060520172\n",
      "epoch 3500/5000, loss = 0.2570025622844696\n",
      "epoch 4000/5000, loss = 0.23144850134849548\n",
      "epoch 4500/5000, loss = 0.2094428986310959\n",
      "epoch 5000/5000, loss = 0.18716368079185486\n"
     ]
    }
   ],
   "source": [
    "model = HMC_LSTM(10, 0.5, [2,1], 2)\n",
    "local_logits, global_logits = model(data)\n",
    "structure = np.array([[0,0,1],[0,0,1],[0,0,0]])\n",
    "alpha = 0.5\n",
    "loss_fn = get_loss_fn(structure, alpha, 0.5)\n",
    "\n",
    "def get_train_fn(model, loss_fn, optimizer):\n",
    "    def train_fn(x,y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = loss_fn(y, model(x))\n",
    "            variables = model.trainable_variables \n",
    "            gradients = tape.gradient(loss, variables)\n",
    "            optimizer.apply_gradients(zip(gradients, variables))\n",
    "        return loss \n",
    "    return train_fn \n",
    "\n",
    "train_fn = get_train_fn(model, loss_fn, tf.keras.optimizers.Adam(1e-2))\n",
    "\n",
    "for epoch in range(5000):\n",
    "    loss = train_fn(data, tf.convert_to_tensor(y,tf.float32))\n",
    "    if (epoch+1)%500 == 0:\n",
    "        print(\"epoch {}/5000, loss = {}\".format(epoch+1, loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_area(x1, y1, x2, y2):\n",
    "    '''\n",
    "    x1 < x2, the inequality should be strict\n",
    "    '''\n",
    "    return (y1 + y2) * (x2 - x1) / 2\n",
    "\n",
    "def get_score(model, x, y):\n",
    "    local_logits, global_logits = model(x)\n",
    "    total_prob, local_prob, global_prob = model.postprocess(local_logits, global_logits)\n",
    "    \n",
    "    precision_list = [0]\n",
    "    recall_list = [1]\n",
    "    for threshold in np.arange(0.01, 1, 0.01):\n",
    "        pred_int = tf.cast(total_prob >= threshold, tf.int16)\n",
    "        tp = pred_int * y \n",
    "        fp = pred_int * (1-y)\n",
    "        fn = (1-pred_int) * y\n",
    "        \n",
    "        tp = tf.reduce_sum(tp)\n",
    "        fp = tf.reduce_sum(fp)\n",
    "        fn = tf.reduce_sum(fn)\n",
    "        \n",
    "        precision = tp / (tp+fp)\n",
    "        recall = tp / (tp+fn)\n",
    "        \n",
    "        if not (np.isnan(precision) or np.isnan(recall)):\n",
    "            precision_list.append(precision)\n",
    "            recall_list.append(recall)\n",
    "        \n",
    "    idx = np.argsort(precision_list)\n",
    "    precision_np = np.array(precision_list)\n",
    "    recall_np = np.array(recall_list)\n",
    "    \n",
    "    precision_np = precision_np[idx]\n",
    "    recall_np = recall_np[idx]\n",
    "    \n",
    "    area = 0 \n",
    "    for i in range(len(precision_np)-1):\n",
    "        area += get_area(precision_np[i], recall_np[i], precision_np[i+1], recall_np[i+1])\n",
    "    \n",
    "    return area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AU(PRC) = 0.9990706177379405\n"
     ]
    }
   ],
   "source": [
    "local_logits, global_logits = model(data)\n",
    "total_prob, local_prob, global_prob = model.postprocess(local_logits, global_logits)\n",
    "print(\"AU(PRC) = {}\".format(get_score(model, data, y)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13 (default, Oct 19 2022, 10:19:43) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b118cd4708a217f676eace989c7f65c28bb1737b4d79cbde311a23c592f7bda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
